# Neural Network PHP

Uma biblioteca completa de Redes Neurais Artificiais implementada em PHP puro, com suporte a m√∫ltiplas arquiteturas, otimizadores e fun√ß√µes de ativa√ß√£o.

## üìã √çndice

- [Caracter√≠sticas](#-caracter√≠sticas)
- [Requisitos](#-requisitos)
- [Instala√ß√£o](#-instala√ß√£o)
- [Uso B√°sico](#-uso-b√°sico)
- [Componentes](#-componentes)
  - [Camadas (Layers)](#camadas-layers)
  - [Fun√ß√µes de Ativa√ß√£o](#fun√ß√µes-de-ativa√ß√£o)
  - [Otimizadores](#otimizadores)
  - [Fun√ß√µes de Perda (Loss)](#fun√ß√µes-de-perda-loss)
- [Configura√ß√£o Avan√ßada](#-configura√ß√£o-avan√ßada)
- [Exemplos Pr√°ticos](#-exemplos-pr√°ticos)
- [Salvamento e Carregamento de Modelos](#-salvamento-e-carregamento-de-modelos)
- [API Completa](#-api-completa)
- [Integra√ß√£o com php_tensor](#-integra√ß√£o-com-php_tensor)
- [Contribuindo](#-contribuindo)
- [Licen√ßa](#-licen√ßa)

## Caracter√≠sticas

- **M√∫ltiplas Camadas**: Dense, Conv2D, Dropout, BatchNormalization, Flatten
- **Fun√ß√µes de Ativa√ß√£o**: Sigmoid, ReLU, LeakyReLU, ELU, Tanh, Softmax, Linear
- **Otimizadores Modernos**: SGD, Adam, AdamW, RMSProp
- **Fun√ß√µes de Perda**: MSE (Mean Squared Error), CrossEntropy
- **Regulariza√ß√£o**: Dropout, BatchNormalization, Weight Decay, Gradient Clipping
- **Early Stopping**: Parada antecipada para evitar overfitting
- **Mini-Batch Training**: Treinamento eficiente com lotes
- **Salvamento/Carregamento**: Persist√™ncia completa de modelos treinados
- **Suporte a Tensor**: Integra√ß√£o opcional com extens√£o `php_tensor` para acelera√ß√£o

## Requisitos

- PHP 8.0 ou superior
- Extens√£o `php_tensor` (opcional, para melhor performance)

## Instala√ß√£o

Clone o reposit√≥rio:

```bash
git clone https://github.com/seu-usuario/Neural-Network-PHP.git
cd Neural-Network-PHP
```

Inclua a classe principal no seu projeto:

```php
require_once 'NeuralNetwork.class.php';
```

## Uso B√°sico

### Exemplo: Problema XOR

```php
<?php
require_once 'NeuralNetwork.class.php';

use NeuralNetwork\Layer\Dense;

// Dataset XOR
$inputs = [
    [0, 0],
    [0, 1],
    [1, 0],
    [1, 1],
];
$targets = [
    [0],
    [1],
    [1],
    [0],
];

// Construir a rede: 2 entradas -> 8 neur√¥nios ocultos -> 1 sa√≠da
$layers = [
    new Dense(2, 8, 'relu'),
    new Dense(8, 1, 'sigmoid'),
];

$nn = new NeuralNetwork($layers);
$nn->configure([
    'learning_rate' => 0.01,
    'optimizer' => 'adam',
    'loss' => 'mse',
]);

// Treinar
$nn->train($inputs, $targets, 10000, 0, true);

// Fazer predi√ß√µes
foreach ($inputs as $input) {
    $output = $nn->predict($input);
    echo "Entrada: [" . implode(', ', $input) . "] => Predi√ß√£o: " . round($output[0]) . "\n";
}
```

## Componentes

### Camadas (Layers)

#### Dense (Camada Totalmente Conectada)

Camada densa tradicional onde cada neur√¥nio est√° conectado a todos os neur√¥nios da camada anterior.

```php
use NeuralNetwork\Layer\Dense;

// Dense(entradas, sa√≠das, fun√ß√£o_ativa√ß√£o)
$layer = new Dense(10, 20, 'relu');
```

**Par√¢metros:**
- `$inputSize` (int): N√∫mero de entradas
- `$outputSize` (int): N√∫mero de neur√¥nios/sa√≠das
- `$activation` (string): Fun√ß√£o de ativa√ß√£o ('relu', 'sigmoid', 'tanh', 'softmax', 'linear', 'leakyrelu', 'elu')

#### Conv2D (Camada Convolucional 2D)

Camada convolucional para processamento de imagens e dados espaciais.

```php
use NeuralNetwork\Layer\Conv2D;

// Conv2D(filtros, kernel_size, stride, padding, ativa√ß√£o)
$layer = new Conv2D(32, 3, 1, 1, 'relu');
```

**Par√¢metros:**
- `$filters` (int): N√∫mero de filtros/kernels
- `$kernelSize` (int): Tamanho do kernel (ex: 3 para 3x3)
- `$stride` (int): Passo da convolu√ß√£o
- `$padding` (int): Padding ao redor da entrada
- `$activation` (string): Fun√ß√£o de ativa√ß√£o

#### Dropout

Regulariza√ß√£o que desativa aleatoriamente neur√¥nios durante o treinamento para prevenir overfitting.

```php
use NeuralNetwork\Layer\Dropout;

// Dropout(taxa_de_dropout)
$layer = new Dropout(0.5); // Desativa 50% dos neur√¥nios
```

**Par√¢metros:**
- `$rate` (float): Taxa de dropout (0.0 a 1.0)

#### BatchNormalization

Normaliza as ativa√ß√µes de uma camada para acelerar o treinamento e melhorar a estabilidade.

```php
use NeuralNetwork\Layer\BatchNormalization;

// BatchNormalization(n√∫mero_de_features)
$layer = new BatchNormalization(64);
```

**Par√¢metros:**
- `$numFeatures` (int): N√∫mero de features a normalizar

#### Flatten

Achata tensores multidimensionais em vetores 1D, √∫til entre camadas convolucionais e densas.

```php
use NeuralNetwork\Layer\Flatten;

$layer = new Flatten();
```

### Fun√ß√µes de Ativa√ß√£o

| Fun√ß√£o | Descri√ß√£o | Uso Recomendado |
|--------|-----------|-----------------|
| **sigmoid** | Sa√≠da entre 0 e 1 | Classifica√ß√£o bin√°ria (camada de sa√≠da) |
| **relu** | max(0, x) | Camadas ocultas (padr√£o) |
| **leakyrelu** | max(0.01x, x) | Camadas ocultas (evita neur√¥nios mortos) |
| **elu** | Exponential Linear Unit | Camadas ocultas (converg√™ncia mais r√°pida) |
| **tanh** | Sa√≠da entre -1 e 1 | Camadas ocultas (dados centrados em zero) |
| **softmax** | Distribui√ß√£o de probabilidade | Classifica√ß√£o multiclasse (camada de sa√≠da) |
| **linear** | Identidade (sem transforma√ß√£o) | Regress√£o (camada de sa√≠da) |

### Otimizadores

#### SGD (Stochastic Gradient Descent)

Otimizador b√°sico com suporte a momentum, weight decay e gradient clipping.

```php
$nn->configure([
    'optimizer' => 'sgd',
    'learning_rate' => 0.01,
    'momentum' => 0.9,
    'weight_decay' => 0.0001,
    'gradient_clip' => 5.0,
]);
```

#### Adam (Adaptive Moment Estimation)

Otimizador adaptativo que combina momentum e RMSProp. Excelente escolha padr√£o.

```php
$nn->configure([
    'optimizer' => 'adam',
    'learning_rate' => 0.001,
]);
```

#### AdamW (Adam with Weight Decay)

Variante do Adam com weight decay desacoplado, melhor para regulariza√ß√£o.

```php
$nn->configure([
    'optimizer' => 'adamw',
    'learning_rate' => 0.001,
    'weight_decay' => 0.01,
    'gradient_clip' => 1.0,
]);
```

#### RMSProp

Otimizador adaptativo que funciona bem com redes recorrentes.

```php
$nn->configure([
    'optimizer' => 'rmsprop',
    'learning_rate' => 0.001,
]);
```

### Fun√ß√µes de Perda (Loss)

#### MSE (Mean Squared Error)

Para problemas de regress√£o.

```php
$nn->configure(['loss' => 'mse']);
```

#### CrossEntropy

Para problemas de classifica√ß√£o.

```php
$nn->configure(['loss' => 'crossentropy']);
```

## ‚öôÔ∏è Configura√ß√£o Avan√ßada

### Exemplo Completo com Todas as Op√ß√µes

```php
use NeuralNetwork\Layer\Dense;
use NeuralNetwork\Layer\BatchNormalization;
use NeuralNetwork\Layer\Dropout;

$layers = [
    new Dense(10, 64, 'relu'),
    new BatchNormalization(64),
    new Dropout(0.3),
    new Dense(64, 32, 'relu'),
    new Dropout(0.2),
    new Dense(32, 1, 'linear'),
];

$nn = new NeuralNetwork($layers);
$nn->configure([
    'learning_rate' => 0.001,      // Taxa de aprendizado
    'optimizer' => 'adamw',         // Otimizador
    'loss' => 'mse',                // Fun√ß√£o de perda
    'batch_size' => 32,             // Tamanho do lote
    'momentum' => 0.9,              // Momentum (para SGD)
    'weight_decay' => 0.01,         // Regulariza√ß√£o L2
    'gradient_clip' => 5.0,         // Gradient clipping
]);

// Treinar com early stopping
$nn->train(
    $inputs,
    $targets,
    epochs: 1000,
    patience: 50,      // Para ap√≥s 50 √©pocas sem melhoria
    verbose: true      // Mostrar progresso
);
```

## Exemplos Pr√°ticos

### 1. Classifica√ß√£o Bin√°ria (XOR)

```php
$inputs = [[0,0], [0,1], [1,0], [1,1]];
$targets = [[0], [1], [1], [0]];

$layers = [
    new Dense(2, 8, 'relu'),
    new Dense(8, 1, 'sigmoid'),
];

$nn = new NeuralNetwork($layers);
$nn->configure(['optimizer' => 'adam', 'loss' => 'mse']);
$nn->train($inputs, $targets, 10000);
```

### 2. Regress√£o (Fun√ß√£o Seno)

```php
$inputs = [];
$targets = [];
for ($i = 0; $i < 100; $i++) {
    $x = $i / 10.0;
    $inputs[] = [$x];
    $targets[] = [sin($x)];
}

$layers = [
    new Dense(1, 32, 'relu'),
    new Dense(32, 32, 'tanh'),
    new Dense(32, 1, 'linear'),
];

$nn = new NeuralNetwork($layers);
$nn->configure(['optimizer' => 'adam', 'loss' => 'mse']);
$nn->train($inputs, $targets, 5000, 0, true);
```

### 3. Classifica√ß√£o Multiclasse

```php
// Dataset Iris simplificado
$inputs = [
    [5.1, 3.5, 1.4, 0.2], // Setosa
    [7.0, 3.2, 4.7, 1.4], // Versicolor
    [6.3, 3.3, 6.0, 2.5], // Virginica
    // ... mais exemplos
];

$targets = [
    [1, 0, 0], // Setosa
    [0, 1, 0], // Versicolor
    [0, 0, 1], // Virginica
    // ... mais exemplos
];

$layers = [
    new Dense(4, 16, 'relu'),
    new Dense(16, 8, 'relu'),
    new Dense(8, 3, 'softmax'),
];

$nn = new NeuralNetwork($layers);
$nn->configure(['optimizer' => 'adam', 'loss' => 'crossentropy']);
$nn->train($inputs, $targets, 1000, 0, true);

// Predi√ß√£o
$prediction = $nn->predict([5.0, 3.0, 1.6, 0.2]);
// Resultado: [0.95, 0.03, 0.02] -> Classe 0 (Setosa)
```

### 4. Rede Convolucional (CNN)

```php
use NeuralNetwork\Layer\Conv2D;
use NeuralNetwork\Layer\Flatten;

$layers = [
    new Conv2D(32, 3, 1, 1, 'relu'),  // 32 filtros 3x3
    new Conv2D(64, 3, 1, 1, 'relu'),  // 64 filtros 3x3
    new Flatten(),
    new Dense(64 * 28 * 28, 128, 'relu'),
    new Dropout(0.5),
    new Dense(128, 10, 'softmax'),
];

$nn = new NeuralNetwork($layers);
$nn->configure([
    'optimizer' => 'adam',
    'loss' => 'crossentropy',
    'batch_size' => 64,
]);
```

## Salvamento e Carregamento de Modelos

### Salvar Modelo Treinado

```php
// Treinar o modelo
$nn->train($inputs, $targets, 1000);

// Salvar em arquivo
$nn->save('modelo_treinado.bin');
```

### Carregar Modelo

```php
// Carregar modelo previamente treinado
$nn = NeuralNetwork::load('modelo_treinado.bin');

// Usar diretamente para predi√ß√µes
$prediction = $nn->predict([1.5, 2.3]);
```

**Nota:** O salvamento preserva:
- Arquitetura completa da rede (camadas)
- Pesos e biases treinados
- Estado do otimizador
- Configura√ß√£o da fun√ß√£o de perda

## API Completa

### Construtor

```php
public function __construct(array $layers, float $learningRate = 0.01)
```

**Par√¢metros:**
- `$layers`: Array de objetos `LayerInterface`
- `$learningRate`: Taxa de aprendizado inicial (padr√£o: 0.01)

### configure()

```php
public function configure(array $options): void
```

**Op√ß√µes dispon√≠veis:**
- `learning_rate` (float): Taxa de aprendizado
- `optimizer` (string): 'sgd', 'adam', 'adamw', 'rmsprop'
- `loss` (string): 'mse', 'crossentropy'
- `batch_size` (int): Tamanho do mini-batch (padr√£o: 32)
- `momentum` (float): Momentum para SGD (padr√£o: 0.0)
- `weight_decay` (float): Regulariza√ß√£o L2 (padr√£o: 0.0)
- `gradient_clip` (float): Valor m√°ximo do gradiente (padr√£o: 0.0 = desabilitado)

### train()

```php
public function train(
    array $inputs,
    array $targets,
    int $epochs,
    int $patience = 0,
    bool $verbose = false
): void
```

**Par√¢metros:**
- `$inputs`: Array de amostras de entrada
- `$targets`: Array de valores alvo correspondentes
- `$epochs`: N√∫mero de √©pocas de treinamento
- `$patience`: √âpocas sem melhoria antes de parar (0 = desabilitado)
- `$verbose`: Exibir progresso do treinamento

**Formato dos dados:**
```php
// Entrada √∫nica
$inputs = [[x1, x2, x3]];
$targets = [[y1, y2]];

// M√∫ltiplas entradas
$inputs = [
    [x1, x2, x3],
    [x1, x2, x3],
    // ...
];
$targets = [
    [y1, y2],
    [y1, y2],
    // ...
];
```

### predict()

```php
public function predict(array $input): array
```

**Par√¢metros:**
- `$input`: Amostra √∫nica ou lote de amostras

**Retorno:**
- Array com as predi√ß√µes

**Exemplos:**
```php
// Predi√ß√£o √∫nica
$output = $nn->predict([1.0, 2.0, 3.0]);
// Retorna: [0.85, 0.15]

// Predi√ß√£o em lote
$outputs = $nn->predict([
    [1.0, 2.0, 3.0],
    [4.0, 5.0, 6.0],
]);
// Retorna: [[0.85, 0.15], [0.23, 0.77]]
```

### save()

```php
public function save(string $filepath): void
```

**Par√¢metros:**
- `$filepath`: Caminho do arquivo para salvar o modelo

### load() (est√°tico)

```php
public static function load(string $filepath): NeuralNetwork
```

**Par√¢metros:**
- `$filepath`: Caminho do arquivo do modelo salvo

**Retorno:**
- Inst√¢ncia de `NeuralNetwork` com o modelo carregado

## Integra√ß√£o com php_tensor

Esta biblioteca suporta a extens√£o `php_tensor` para acelera√ß√£o de opera√ß√µes matriciais. Se a extens√£o estiver instalada, as opera√ß√µes ser√£o automaticamente aceleradas.

### Verificar Suporte

```php
if (extension_loaded('tensor')) {
    echo "php_tensor est√° dispon√≠vel - usando acelera√ß√£o!\n";
} else {
    echo "Usando implementa√ß√£o PHP pura\n";
}
```

### Instala√ß√£o do php_tensor

Consulte a documenta√ß√£o oficial do php_tensor para instru√ß√µes de instala√ß√£o espec√≠ficas do seu sistema operacional.

## Dicas de Uso

### Escolhendo a Taxa de Aprendizado

- **Muito alta**: Modelo n√£o converge, loss oscila
- **Muito baixa**: Treinamento muito lento
- **Recomendado**: 
  - SGD: 0.01 - 0.1
  - Adam/AdamW: 0.001 - 0.01
  - RMSProp: 0.001 - 0.01

### Prevenindo Overfitting

1. **Dropout**: Adicione camadas de Dropout (0.2 - 0.5)
2. **Weight Decay**: Use regulariza√ß√£o L2 (0.0001 - 0.01)
3. **Early Stopping**: Configure `patience` no treinamento
4. **Batch Normalization**: Adicione entre camadas densas
5. **Mais dados**: Aumente o dataset de treinamento

### Normaliza√ß√£o de Dados

Sempre normalize seus dados de entrada:

```php
// Min-Max Normalization (0-1)
function normalize($value, $min, $max) {
    return ($value - $min) / ($max - $min);
}

// Standardization (m√©dia 0, desvio 1)
function standardize($value, $mean, $std) {
    return ($value - $mean) / $std;
}
```

### Debugging

```php
// Ativar verbose para ver o progresso
$nn->train($inputs, $targets, 1000, 0, verbose: true);

// Verificar predi√ß√µes durante o treinamento
for ($epoch = 0; $epoch < 100; $epoch++) {
    $nn->train($inputs, $targets, 10, 0, false);
    
    $testPred = $nn->predict($testInput);
    echo "√âpoca $epoch: Predi√ß√£o = " . $testPred[0] . "\n";
}
```
